{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets setup some configuration variables for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Poetry to setup the python environment and dependancies\n",
    "# https://python-poetry.org/docs/basic-usage/\n",
    "\n",
    "# Run the following command to install the dependancies\n",
    "# poetry install\n",
    "\n",
    "\n",
    "# Define the path for the data file (csv) as exported from Survey Monkey\n",
    "data_folder = '../survey_data/'\n",
    "\n",
    "csv_survey_file = data_folder + 'surveydata.csv'  # Replace with desired input file path\n",
    "csv_survey_merged_output = data_folder + 'survey_data_merged.csv'  # Replace with desired output file path\n",
    "\n",
    "#import os\n",
    "#print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Assuming you downloaded the CSV from SurveyMonkey lets prep the data\n",
    "\n",
    "### 1.1. First we merge the column options into a single column with a semicolon separator for each option\n",
    "\n",
    "| Name | Favorite color |  |  | Age | Food Likes |  |  |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|  | Blue | Red | Yellow |  | Meat | Veg | Fish |\n",
    "| John |  | Red |  | 30 |  | Veg | Fish |\n",
    "| Mary | Blue |  |  | 20 | Meat |  |  |\n",
    "| June |  |  | Yellow | 43 | Meat | Veg |\n",
    "\n",
    "We want to have this:\n",
    "\n",
    "| Name | Favorite color | Age | Food Likes |\n",
    "| --- | --- | --- | --- |\n",
    "| John | Red | 30 | Veg; Fish |\n",
    "| Mary | Blue | 20 | Meat |\n",
    "| June | Yellow | 43 | Meat; Veg |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv(file_path, output_file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "    # Headers\n",
    "    headers = df.iloc[0]\n",
    "\n",
    "    # Identify start and end indices of each group of option columns\n",
    "    option_indices = []\n",
    "    start_index = None\n",
    "    for i, header in enumerate(headers):\n",
    "        if pd.isna(header):\n",
    "            if start_index is None:\n",
    "                start_index = i - 1\n",
    "        elif start_index is not None:\n",
    "            option_indices.append((start_index, i))\n",
    "            start_index = None\n",
    "    if start_index is not None:\n",
    "        option_indices.append((start_index, len(headers)))\n",
    "\n",
    "    # Process data rows\n",
    "    processed_data = []\n",
    "    for i in range(2, len(df)):\n",
    "        row = df.iloc[i]\n",
    "        new_row = []\n",
    "        last_index = 0\n",
    "        for start, end in option_indices:\n",
    "            # Add non-option columns as they are\n",
    "            new_row.extend(row[last_index:start])\n",
    "            # Merge option columns\n",
    "            values = row[start:end].dropna()\n",
    "            concatenated_values = '; '.join(values.astype(str))\n",
    "            new_row.append(concatenated_values)\n",
    "            last_index = end\n",
    "\n",
    "        # Add remaining columns if any\n",
    "        if last_index < len(headers):\n",
    "            new_row.extend(row[last_index:])\n",
    "\n",
    "        processed_data.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame for output\n",
    "    new_headers = [header for header in headers if pd.notna(header)]\n",
    "    output_df = pd.DataFrame(processed_data, columns=new_headers)\n",
    "    output_df.to_csv(output_file_path, index=False)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "# Process the CSV\n",
    "output_df = process_csv(csv_survey_file, csv_survey_merged_output)\n",
    "print(output_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The respondant IDs can have issues from CSV export with Survey Monkey\n",
    "So we wan tto use the row number as the ID so replace the first column with the row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the first column values with the row number isntead of the Survey Monkey repondant ID which exports as a number in scientific notation\n",
    "output_df['Respondent ID'] = output_df.index + 1\n",
    "print(output_df.head())\n",
    "# overwrite the output file\n",
    "output_df.to_csv(csv_survey_merged_output, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Now lets work on the data header columns, using AI to make the long questions into \"codes\" or short titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section uses AI, in paritcular we are using OpenAI's GPT-3.5 latest model\n",
    "The cost is (as of Jan 2024), $0.0001 per 1000 tokens which means if a prompt has say 50 words the data is 100 words, and and prompt response is 100 words, then thats 250 words (350 tokens). Say there are 200 survey entries, thats 350x200 = 70,000 tokens. So thats $0.007 to run a batch of 200 surveys.\n",
    "\n",
    "You will need the API key in the `.env` file as `OPENAI_API_KEY`\n",
    "\n",
    "We also use `langchain` to do all the LLM interactions, its just easier.\n",
    "It will get installed with the initil `poetry install` command.\n",
    "\n",
    "https://python.langchain.com/docs/get_started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the survey data file we want to use\n",
    "csv_survey = data_folder + 'survey_data_merged.csv'  # Replace with desired output file path\n",
    "\n",
    "# The AI language model\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "# get the API key from the environment variable\n",
    "import os\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating shorter question titles\n",
    "\n",
    "Lets dump all the questions, one per line, to a language model and have it come up with shorter titles for each column.\n",
    "making sure they are all unique. Then return the full data with better titles.\n",
    "We will use CamelCase for the titles, with max 5 words per title.\n",
    "\n",
    "This can take 10 seconds as the Language model writes it all out\n",
    "\n",
    "We will end up with a new CSV file where the column titls are shorter and more readable.\n",
    "We will also create a map for the short titles to the long titles as we might need the long titles (questions) later for the language model to understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets dump all the questions, one per line, to a language model and have it come up with shorter titles for each column.\n",
    "# making sure they are all unique. Then return the full data with better titles.\n",
    "# We will use CamelCase for the titles, with max 5 words per title.\n",
    "\n",
    "\n",
    "# define the prompt\n",
    "tpl = \"\"\" \n",
    "You will be given the questions from a survey, one per line and you will need to come up with a shorter title for each question.\n",
    "Use CamelCase for the titles, with max 5 words per title.\n",
    "Only provide the new titles, one per line, in the same order as the questions.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\"\"\"\n",
    "\n",
    "# get the questions from the df from the survey data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(csv_survey)\n",
    "questions = df.columns.values.tolist()\n",
    "# list as one per line\n",
    "questions = \"\\n\".join(questions)\n",
    "# print(questions)\n",
    "\n",
    "# Create a prompt with the template\n",
    "prompt_tpl = PromptTemplate.from_template(tpl)\n",
    "messages = [ HumanMessage(content=prompt_tpl.format(questions=questions)) ]\n",
    "chat.invoke(messages)\n",
    "\n",
    "resp = chat.batch([messages])\n",
    "\n",
    "# get the list\n",
    "titles = resp[0].content.split(\"\\n\")\n",
    "\n",
    "# make sure the line count is the same\n",
    "assert len(titles) == len(questions.split(\"\\n\"))\n",
    "\n",
    "# now create a map for the short Titles to the long questions and write to a json file\n",
    "shortTitles = {}\n",
    "for i in range(len(titles)):\n",
    "    shortTitles[titles[i]] = questions.split(\"\\n\")[i]\n",
    "\n",
    "# write to a json file\n",
    "import json\n",
    "with open(data_folder + 'survey_data_titleMap.json', 'w') as outfile:\n",
    "    json.dump(shortTitles, outfile)\n",
    "\n",
    "\n",
    "# replace the titles in the df\n",
    "df.columns = titles\n",
    "df.head()\n",
    "\n",
    "# write to a new csv with shortTitles\n",
    "csv_survey_shortTitles = data_folder + 'survey_data_shortTitles.csv'  # Replace with desired output file path\n",
    "df.to_csv(csv_survey_shortTitles, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
