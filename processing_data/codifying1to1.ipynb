{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows how to codify **open question** responses\n",
    "This is so we can better group respondants together using facets.\n",
    "\n",
    "*In this version, we want to make sure we code 1 answer to 1 code.. versus look at all the answers of that respondnt within that question and come up with a few codes.*\n",
    "\n",
    "This notebook will cover the process for 1 field to codify, you can repeat it for other fields.\n",
    "It will created the codes, store a file with the codes and then use those to code EACH response.\n",
    "\n",
    "The file will be a csv that will include the unique respondant identify and the new field\n",
    "So that it can be used for typical `VLOOKUP` in sheets or `MERGE` in pandas if you want to use python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the AI side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the data file (csv) as exported from Survey Monkey\n",
    "data_folder = '../survey_data/'\n",
    "# Define the survey data file\n",
    "# here we use the shortTitles version of the survey data (see data preparation)\n",
    "csv_survey = data_folder + 'survey_data_shortTitles.csv'  # Replace with desired output file path\n",
    "\n",
    "# The AI language model\n",
    "model =  \"gpt-4-0125-preview\"; # \"gpt-3.5-turbo-1106\" # \n",
    "\n",
    "# get the API key from the environment variable\n",
    "import os\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create codes for each of those *open* questions\n",
    "\n",
    "In our survey we have a few questions that are open that need to be codified.\n",
    "So lets create a JSON file with the list of codes and their descriptions using AI by showing the AI all the responses for that question and some \"meta data\" about that respondant (optional) to help the AI create better codes.\n",
    "\n",
    "For our example we are going to codify the \"Key responsibilities\" question. Ie what are the key responsibilities of the role.\n",
    "And use the Job Titles and Job Title, Role in Rural water sector as the meta data to help the AI come up with a good contextualized list.\n",
    "\n",
    "So for example we will inject into the prompt\n",
    "\n",
    "```\n",
    "Key Responsibilities: .....\n",
    "Job Titles: Project Manager\n",
    "Job Title: WASH & NUT Field Manager\n",
    "Role in Rural water sector: Project Manager\n",
    "----\n",
    "(repeat for all responses)\n",
    "```\n",
    "\n",
    "We will later use those codes to apply to each of the rows in the survey data.\n",
    "\n",
    "You can set `SAMPLE_ONLY` if you only want to test this with a few responses, otherwise it will use all the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ====================================\n",
    "# USE SAMPLE ONLY \n",
    "# Set to true to only do a sample of the responses.. to test \n",
    "SAMPLE_ONLY = False\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# Change these to the fields you want to use \n",
    "#\n",
    "field_to_code = \"CountryHelpQuestions\" # this must exist as a column in the survey data\n",
    "coded_field = \"CountryHelpQuestions_Coded\" # this is a new colum that will be added to the survey data\n",
    "unique_id = \"RespondentId\" # unique id for each response in the survey\n",
    "number_of_codes = 15\n",
    "# provide some context\n",
    "context = \"In the context of a Survey of WASH practitioners responding to a survey about capacity building needs, this concerns questions that team members ask the respondant.\"\n",
    "#==================================================================================================\n",
    "\n",
    "# make sure all those fields exist in the survey data and raise an error if not\n",
    "df = pd.read_csv(csv_survey)\n",
    "for field in [field_to_code, unique_id]:\n",
    "    if field not in df.columns:\n",
    "        print(f\"ERROR: field '{field}' not found in the survey data\")\n",
    "        raise ValueError(f\"ERROR: field '{field}' not found in the survey data. Maybe the column names changes or you are using the wrong csv file\")\n",
    "\n",
    "\n",
    "# for each response create a string with field: value\\n for code_to_field and all fields in fields_to_help\n",
    "data_string = \"\"\n",
    "\n",
    "# a df_with_data will only contain the responses that have data in the field_to_code and fields_to_help\n",
    "df_with_data = df[~pd.isna(df[field_to_code])]\n",
    "\n",
    "# How many responses with data\n",
    "print(f\"Number of responses with data: {len(df_with_data)}\")\n",
    "\n",
    "count = 0\n",
    "for index,row in df_with_data.iterrows():\n",
    "    # check if we are only doing a sample\n",
    "    if SAMPLE_ONLY and count > 10:\n",
    "        break\n",
    "    count += 1\n",
    "    data_string += f\"ID: {row[unique_id]}\\n\"\n",
    "    data_string += f\"{field_to_code}: {row[field_to_code]}\\n\"\n",
    "    data_string += \"-----\\n\"\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\",f\"Number of reponses with data used: {len(data_string.split('-----'))-1}\",\"\\n\\n\\n\")\n",
    "\n",
    "# print(data_string[:4000])\n",
    "\n",
    "\n",
    "# define the prompt\n",
    "tpl = \"\"\" \n",
    "You are an expert in analyzing data. {context}.You will be given all the anwsers to the question \"{field_to_code}\".\n",
    "Come up with {number_of_codes} short labels for the answers to the question \"{field_to_code}\" based on all the other answers in the survey.\n",
    "\n",
    "List of answers to {field_to_code} in context: \n",
    "===\n",
    "\n",
    "{data_string}\n",
    "\n",
    "===\n",
    "The codes should be a descriptive word group of 4 to 6 words each.\n",
    "ONLY return the {number_of_codes} codes, one per line, and add a description for it in 40 words, nothing else. For example:\n",
    "\n",
    "Word Group Code: This a description of the code in 30 words.\n",
    "Another Code Here: This a another description of the code in 30 words.\n",
    "\n",
    "The codes must be a normal sentence ie Water Management, not WaterManagement NEVER USE snake_case or camelCase for the codes.\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt with the template\n",
    "prompt_tpl = PromptTemplate.from_template(tpl)\n",
    "content = prompt_tpl.format(field_to_code=field_to_code, data_string=data_string, context=context, number_of_codes=number_of_codes)\n",
    "\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets send the request to the AI to create the codes.\n",
    "And save the codes to a file (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# send to the LLM\n",
    "messages = [ HumanMessage(content=content) ]\n",
    "\n",
    "chat.invoke(messages)\n",
    "resp = chat.batch([messages])\n",
    "print(resp[0].content)\n",
    "\n",
    "# write a json file splitting the response (Code: Description) into codes and descriptions as  { code, description }\n",
    "import json\n",
    "codes = []\n",
    "for line in resp[0].content.split(\"\\n\"):\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    code, description = line.split(\": \")\n",
    "    # remove any numbering part of the string like 1. using regex for the id and trim\n",
    "    code = re.sub(r\"^\\d+\\.\\s*\", \"\", code).strip()\n",
    "    codes.append({\"code\": code, \"description\": description.strip()})\n",
    "\n",
    "with open(f\"{data_folder}{field_to_code}_codes.json\", \"w\") as f:\n",
    "    json.dump(codes, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply the codes to each of the responses\n",
    "\n",
    "Now we need to apply the codes to each of the responses. We will also use AI for that.\n",
    "\n",
    "To save time and tokens we should apply the codes in batches so we don't repeat all the code options for each call.\n",
    "\n",
    "We will have it return just the unique id of the response and the code that was applied to it.\n",
    "\n",
    "Then we will save that to a file (CSV) so we can use it in sheets or pandas to merge it with the original data.\n",
    "\n",
    "The filename starts with \"Lookup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# ====================================\n",
    "# USE SAMPLE ONLY \n",
    "# Set to true to only do a sample of the responses.. to test \n",
    "SAMPLE_ONLY = False\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# we are reusing the codes from the previous step, read the file\n",
    "with open(f\"{data_folder}{field_to_code}_codes.json\", \"r\") as f:\n",
    "    codes = json.load(f)\n",
    "          \n",
    "print(f\"Number of codes: {len(codes)}\")\n",
    "\n",
    "# in batches of 5 responses, collate the data that is used but the LLM to guess what the best code is for each response\n",
    "# reuse the data_string from above\n",
    "responses = data_string.split(\"-----\")\n",
    "print(f\"Number of responses: {len(responses)}\")\n",
    "\n",
    "# create an AI process to generate the code\n",
    "# create the prompt\n",
    "tpl = \"\"\" \n",
    "You are an expert in coding data. {context}. You will be given codes to choose from, to code open text answers separated by semi-colons, for e.g.\n",
    "\n",
    "{field_to_code}: something; something else; more here; and again; \n",
    "\n",
    "You will be given a batch of answers, each line with have an ID.\n",
    "You can ONLY choose from the codes provided to assign to each part of the answer. Assign a code for each part. So if you get 1, only give 1, if you get 2, then only give 2, and so on. Repeat the code if necessary.\n",
    "Return the codes you think best describes each answer, as one per line, codes separated by semi-colon in the format ID; Code1; Code2;, Code3;.. etc. Without the description. For example: \n",
    "\n",
    "54; Coded something; Coded something else; ...\n",
    "\n",
    "Ensure the coded answers are representative of the answers given. If you are not sure, leave it blank. \n",
    "List of Answers:\n",
    "{batch_of_answers}\n",
    "\n",
    "List of Codes and their descriptions you have to choose from:\n",
    "{codes}\n",
    "\n",
    "Never ever make up a code, only use the codes provided.\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt with the template\n",
    "prompt_tpl = PromptTemplate.from_template(tpl)\n",
    "\n",
    "# create a fuynction we can use for each batch\n",
    "\n",
    "\n",
    "def get_codes_for_this_batch(batch_of_answers):\n",
    "    # get codes as 1 per line with Code: Description from codes\n",
    "    codes_list = \"\\n\".join(\n",
    "        [f\"{code['code']}: {code['description']}\" for code in codes])\n",
    "    content = prompt_tpl.format(\n",
    "        batch_of_answers=batch_of_answers, codes=codes_list, field_to_code=field_to_code, context=context)\n",
    "    print(content)\n",
    "    # send to the LLM\n",
    "    messages = [HumanMessage(content=content)]\n",
    "    chat.invoke(messages)\n",
    "    resp = chat.batch([messages])\n",
    "    print(resp[0].content)\n",
    "    # split id: code and return array of arrays\n",
    "    return [line.split(\"; \") for line in resp[0].content.split(\"\\n\") if line.strip() != \"\"]\n",
    "# split responses in batches of 5 and process through the LLM\n",
    "all_reponses = []\n",
    "per_batch = 5\n",
    "## print \"Doing sample only\" if SAMPLE_ONLY is True\n",
    "if SAMPLE_ONLY:\n",
    "    print(\"Doing sample only of 3 batches or 15 responses\")\n",
    "\n",
    "## if smaple only, only do 3 batches else do all\n",
    "for i in range(0, SAMPLE_ONLY and 15 or len(responses), per_batch):\n",
    "    batch_of_answers = \"\\n\".join(responses[i:i+5])\n",
    "    # number batch as int\n",
    "    print(f\"Batch {int(i/per_batch)+1}: {len(responses[i:i+5])}\")\n",
    "    this_batch = get_codes_for_this_batch(batch_of_answers)\n",
    "    # merge into all_reponses\n",
    "    all_reponses += this_batch\n",
    "\n",
    "# print(all_reponses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_reponses)\n",
    "# now create a CSV file with the response id's and codes from all_responses and map the id to unique_id field and code to coded_field\n",
    "# given array of arrays we can use pandas to create a dataframe\n",
    "\n",
    "# In case the respnse has more than 1 code column, add a new column for each code using coded_field name with number 1, 2, 3, etc. as suffix\n",
    "# check column count then create a dataframe using above\n",
    "columns = [unique_id]\n",
    "\n",
    "# iterative over the columns in the responses to find with one with the most columns\n",
    "columns_in_responses = 0\n",
    "for response in all_reponses:\n",
    "    columns_in_responses = max(columns_in_responses, len(response))\n",
    "\n",
    "print(f\"Number of columns in responses: {columns_in_responses}\")\n",
    "\n",
    "for i in range(columns_in_responses-1):\n",
    "    columns.append(f\"{coded_field}_{i+1}\")\n",
    "\n",
    "df_codes = pd.DataFrame(all_reponses, columns=columns)\n",
    "# export as csv\n",
    "df_codes.to_csv(f\"{data_folder}lookup_{field_to_code}_codes.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
