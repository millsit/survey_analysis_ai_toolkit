{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows how to codify **open question** responses\n",
    "This is so we can better group respondants together using facets\n",
    "\n",
    "This notebook will cover the process for 1 field to codify, you can repeat it for other fields.\n",
    "It will created the codes, store a file with the codes and then use those to code EACH response.\n",
    "\n",
    "The file will be a csv that will include the unique respondant identify and the new field\n",
    "So that it can be used for typical `VLOOKUP` in sheets or `MERGE` in pandas if you want to use python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the AI side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the data file (csv) as exported from Survey Monkey\n",
    "data_folder = '../survey_data/'\n",
    "# Define the survey data file\n",
    "# here we use the shortTitles version of the survey data (see data preparation)\n",
    "csv_survey = data_folder + 'survey_data_shortTitles.csv'  # Replace with desired output file path\n",
    "\n",
    "# The AI language model\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "# get the API key from the environment variable\n",
    "import os\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create codes for each of those *open* questions\n",
    "\n",
    "In our survey we have a few questions that are open that need to be codified.\n",
    "So lets create a JSON file with the list of codes and their descriptions using AI by showing the AI all the responses for that question and some \"meta data\" about that respondant (optional) to help the AI create better codes.\n",
    "\n",
    "For our example we are going to codify the \"Key responsibilities\" question. Ie what are the key responsibilities of the role.\n",
    "And use the Job Titles and Job Title, Role in Rural water sector as the meta data to help the AI come up with a good contextualized list.\n",
    "\n",
    "So for example we will inject into the prompt\n",
    "\n",
    "```\n",
    "Key Responsibilities: .....\n",
    "Job Titles: Project Manager\n",
    "Job Title: WASH & NUT Field Manager\n",
    "Role in Rural water sector: Project Manager\n",
    "----\n",
    "(repeat for all responses)\n",
    "```\n",
    "\n",
    "We will later use those codes to apply to each of the rows in the survey data.\n",
    "\n",
    "You can set `SAMPLE_ONLY` if you only want to test this with a few responses, otherwise it will use all the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ====================================\n",
    "# USE SAMPLE ONLY \n",
    "# Set to true to only do a sample of the responses.. to test \n",
    "SAMPLE_ONLY = False\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# Change these to the fields you want to use \n",
    "#\n",
    "field_to_code = \"KeyResponsibilities\" # this must exist as a column in the survey data\n",
    "coded_field = \"KeyResponsibilities_Coded\" # this is a new colum that will be added to the survey data\n",
    "unique_id = \"RespondentId\" # unique id for each response in the survey\n",
    "# other fields to help the model pick a good code label for answers\n",
    "## make sure they all exist!\n",
    "fields_to_help = [\"JobTitles\", \"JobTitle\", \"RoleInRuralWaterSector\"]\n",
    "number_of_codes = 15\n",
    "\n",
    "#==================================================================================================\n",
    "\n",
    "# as list\n",
    "list_help_fields = \", \".join(fields_to_help)\n",
    "\n",
    "# make sure all those fields exist in the survey data and raise an error if not\n",
    "df = pd.read_csv(csv_survey)\n",
    "for field in [field_to_code, unique_id] + fields_to_help:\n",
    "    if field not in df.columns:\n",
    "        print(f\"ERROR: field '{field}' not found in the survey data\")\n",
    "        raise ValueError(f\"ERROR: field '{field}' not found in the survey data. Maybe the column names changes or you are using the wrong csv file\")\n",
    "\n",
    "\n",
    "# for each response create a string with field: value\\n for code_to_field and all fields in fields_to_help\n",
    "data_string = \"\"\n",
    "\n",
    "# a df_with_data will only contain the responses that have data in the field_to_code and fields_to_help\n",
    "df_with_data = df[~pd.isna(df[field_to_code]) & ~pd.isna(df[fields_to_help]).all(axis=1)]\n",
    "\n",
    "# How many responses with data\n",
    "print(f\"Number of responses with data: {len(df_with_data)}\")\n",
    "\n",
    "count = 0\n",
    "for index,row in df_with_data.iterrows():\n",
    "    # check if we are only doing a sample\n",
    "    if SAMPLE_ONLY and count > 10:\n",
    "        break\n",
    "    count += 1\n",
    "    data_string += f\"ID: {row[unique_id]}\\n\"\n",
    "    data_string += f\"{field_to_code}: {row[field_to_code]}\\n\"\n",
    "    for field in fields_to_help:\n",
    "        data_string += f\"{row[field]}\\n\" ##  f\"{field}: {row[field]}\\n\"  # removing field to save tokens\n",
    "    data_string += \"-----\\n\"\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\",f\"Number of reponses with data used: {len(data_string.split('-----'))-1}\",\"\\n\\n\\n\")\n",
    "\n",
    "print(data_string[:4000])\n",
    "\n",
    "\n",
    "# define the prompt\n",
    "tpl = \"\"\" \n",
    "You are an expert in analyzing data. You will be given all the anwsers to the question \"{field_to_code}\" in the context of the respondants answer to {list_help_fields}.\n",
    "Come up with {number_of_codes} short labels for the answers to the question \"{field_to_code}\" based on all the other answers in the survey.\n",
    "\n",
    "List of answers to {field_to_code} in context: \n",
    "===\n",
    "\n",
    "{data_string}\n",
    "\n",
    "===\n",
    "The codes should a descriptive word group of 4 to 6 words each.\n",
    "ONLY return the {number_of_codes} codes, one per line, and add a description for it in 40 words, nothing else. For example:\n",
    "\n",
    "Word Group Code: This a description of the code in 30 words. It can be longer but not shorter.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt with the template\n",
    "prompt_tpl = PromptTemplate.from_template(tpl)\n",
    "content = prompt_tpl.format(field_to_code=field_to_code, data_string=data_string, list_help_fields=list_help_fields, number_of_codes=number_of_codes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets send the request to the AI to create the codes.\n",
    "And save the codes to a file (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# send to the LLM\n",
    "messages = [ HumanMessage(content=content) ]\n",
    "\n",
    "chat.invoke(messages)\n",
    "resp = chat.batch([messages])\n",
    "print(resp[0].content)\n",
    "\n",
    "# write a json file splitting the response (Code: Description) into codes and descriptions as  { code, description }\n",
    "import json\n",
    "codes = []\n",
    "for line in resp[0].content.split(\"\\n\"):\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    code, description = line.split(\": \")\n",
    "    # remove any numbering part of the string like 1. using regex for the id and trim\n",
    "    code = re.sub(r\"^\\d+\\.\\s*\", \"\", code).strip()\n",
    "    codes.append({\"code\": code, \"description\": description.strip()})\n",
    "\n",
    "with open(f\"{data_folder}{field_to_code}_codes.json\", \"w\") as f:\n",
    "    json.dump(codes, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply the codes to each of the responses\n",
    "\n",
    "Now we need to apply the codes to each of the responses. We will also use AI for that.\n",
    "\n",
    "To save time and tokens we should apply the codes in batches so we don't repeat all the code options for each call.\n",
    "\n",
    "We will have it return just the unique id of the response and the code that was applied to it.\n",
    "\n",
    "Then we will save that to a file (CSV) so we can use it in sheets or pandas to merge it with the original data.\n",
    "\n",
    "The filename starts with \"Lookup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# USE SAMPLE ONLY \n",
    "# Set to true to only do a sample of the responses.. to test \n",
    "SAMPLE_ONLY = False\n",
    "# ==========================================\n",
    "\n",
    "# we are reusing the codes from the previous step\n",
    "print(f\"Number of codes: {len(codes)}\")\n",
    "\n",
    "# in batches of 5 responses, collate the data that is used but the LLM to guess what the best code is for each response\n",
    "# reuse the data_string from above\n",
    "responses = data_string.split(\"-----\")\n",
    "print(f\"Number of responses: {len(responses)}\")\n",
    "\n",
    "# create an AI process to generate the code\n",
    "# create the prompt\n",
    "tpl = \"\"\" \n",
    "You are an expert in coding data. You will be given codes to choose from. You can ONLY choose from those codes. \n",
    "You will be given a batch of answers which each have an ID.\n",
    "Return the code you think best describes each answer, as one per line, in the format ID: Code. Without the description. For example: \n",
    "54: Water Management\n",
    "\n",
    "List of Answers:\n",
    "{batch_of_answers}\n",
    "\n",
    "List of Codes and their descriptions you can choose from:\n",
    "{codes}\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt with the template\n",
    "prompt_tpl = PromptTemplate.from_template(tpl)\n",
    "\n",
    "# create a fuynction we can use for each batch\n",
    "\n",
    "\n",
    "def get_codes_for_this_batch(batch_of_answers):\n",
    "    # get codes as 1 per line with Code: Description from codes\n",
    "    codes_list = \"\\n\".join(\n",
    "        [f\"{code['code']}: {code['description']}\" for code in codes])\n",
    "    content = prompt_tpl.format(\n",
    "        batch_of_answers=batch_of_answers, codes=codes_list)\n",
    "    print(content)\n",
    "    # send to the LLM\n",
    "    messages = [HumanMessage(content=content)]\n",
    "    chat.invoke(messages)\n",
    "    resp = chat.batch([messages])\n",
    "    print(resp[0].content)\n",
    "    # split id: code and return array of arrays\n",
    "    return [line.split(\": \") for line in resp[0].content.split(\"\\n\") if line.strip() != \"\"]\n",
    "# split responses in batches of 5 and process through the LLM\n",
    "all_reponses = []\n",
    "per_batch = 5\n",
    "## print \"Doing sample only\" if SAMPLE_ONLY is True\n",
    "if SAMPLE_ONLY:\n",
    "    print(\"Doing sample only of 3 batches or 15 responses\")\n",
    "\n",
    "## if smaple only, only do 3 batches else do all\n",
    "for i in range(0, SAMPLE_ONLY and 15 or len(responses), per_batch):\n",
    "    batch_of_answers = \"\\n\".join(responses[i:i+5])\n",
    "    # number batch as int\n",
    "    print(f\"Batch {int(i/per_batch)+1}: {len(responses[i:i+5])}\")\n",
    "    this_batch = get_codes_for_this_batch(batch_of_answers)\n",
    "    # merge into all_reponses\n",
    "    all_reponses += this_batch\n",
    "\n",
    "# print(all_reponses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_reponses)\n",
    "# now create a CSV file with the response id's and codes from all_responses and map the id to unique_id field and code to coded_field\n",
    "# given array of arrays we can use pandas to create a dataframe\n",
    "df_codes = pd.DataFrame(all_reponses, columns=[unique_id, coded_field])\n",
    "# export as csv\n",
    "df_codes.to_csv(f\"{data_folder}lookup_{field_to_code}_codes.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
